{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0434a0c9-c136-4a83-aea8-3e68feb8117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2880c411-d7d1-441d-ab78-6b51ac0a9afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ERA5 calculation functions\n",
    "\n",
    "# Function to get 2D variable data\n",
    "def calc_2D_var(data_path,lati,loni,datei):\n",
    "\n",
    "    data = xr.open_dataset(data_path)\n",
    "\n",
    "    varname = list(data.keys())[0]\n",
    "    data = data[varname]\n",
    "    \n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(data.time.values)-date_i\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))\n",
    "\n",
    "    # Get lon index within each file\n",
    "    diff_lon = data.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = data.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    var = data[int(idt[0]),int(idlat[0]),int(idlon[0])].values\n",
    "    \n",
    "    return var\n",
    "            \n",
    "# Function to calculate the mean specific humidity between two pressure levels\n",
    "def calc_mean_layer_sh(alt_arr,data_path,lati,loni,datei):\n",
    "\n",
    "    # Open up data with xarray\n",
    "    data = xr.open_dataset(data_path)\n",
    "\n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(data.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))\n",
    "            \n",
    "    # Get lon index within each file\n",
    "    diff_lon = data.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = data.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    # Get pressure level indicies\n",
    "    alt0 = np.where(np.abs(np.array(data.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(data.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(data.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(data.level[:].values) - alt_arr[1])))\n",
    "    #if n == 0:\n",
    "    #    print(alt0[0],alt1[0])\n",
    "\n",
    "    # Get specific data and do calculation\n",
    "    var1 = data.q[int(idt[0]),int(alt0[0]):int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var = np.nanmean(var1)\n",
    "    del(var1)\n",
    "\n",
    "    data.close()\n",
    "\n",
    "    return var\n",
    "        \n",
    "# Function to calculate the mean lapse rate between two pressure levels ( K/km )\n",
    "def calc_mean_lapse_rate(alt_arr,t_data_path,z_data_path,lati,loni,datei):\n",
    "\n",
    "    g = 9.807 #m/s^2\n",
    "\n",
    "    # Open up data with xarray\n",
    "    tdata = xr.open_dataset(t_data_path)\n",
    "    zdata = xr.open_dataset(z_data_path)\n",
    "\n",
    "    \n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(tdata.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))    \n",
    "    \n",
    "    # Get lon index within each file\n",
    "    diff_lon = tdata.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = tdata.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    # Get pressure level indicies\n",
    "    alt0 = np.where(np.abs(np.array(tdata.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(tdata.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(tdata.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(tdata.level[:].values) - alt_arr[1])))\n",
    "    #if n == 0:\n",
    "    #    print(alt0[0],alt1[0])\n",
    "\n",
    "    # Get specific data and do calculation\n",
    "    var1 = tdata.t[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var2 = tdata.t[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "\n",
    "    var3 = zdata.z[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var4 = zdata.z[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "\n",
    "    var = (var1-var2)/((var3-var4)/g/1000)\n",
    "    del(var1,var2,var3,var4)\n",
    "\n",
    "    tdata.close()\n",
    "    zdata.close()\n",
    "\n",
    "    return var\n",
    "    \n",
    "\n",
    "# Function to calculate the wind shear between two pressure levels\n",
    "def calc_layer_shear(alt_arr,data_path_u,data_path_v,lati,loni,datei):\n",
    "\n",
    "    #### LOW LEVEL SHEAR        \n",
    "#    udata = xr.open_dataset(inpath+upath+era5_datestr+'_'+hemi+'.nc')\n",
    "#    vdata = xr.open_dataset(inpath+vpath+era5_datestr+'_'+hemi+'.nc')\n",
    "    udata = xr.open_dataset(data_path_u)\n",
    "    vdata = xr.open_dataset(data_path_v)\n",
    "\n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(udata.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))    \n",
    "\n",
    "    # Get lon index within each file\n",
    "    diff_lon = udata.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = vdata.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))   \n",
    "    \n",
    "#    alt_arr = [800, 1000]\n",
    "    alt0 = np.where(np.abs(np.array(udata.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(udata.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(udata.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(udata.level[:].values) - alt_arr[1])))\n",
    "    if n == 0:\n",
    "        print(alt0[0],alt1[0])\n",
    "\n",
    "    var1 = udata.u[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var2 = vdata.v[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var3 = udata.u[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var4 = vdata.v[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var = np.sqrt(np.power((var3-var1),2.0) + np.power((var4-var2),2.0))\n",
    "    #lls_vals = np.append(lls_vals,var)       \n",
    "    \n",
    "    del(var1,var2,var3,var4)\n",
    "    udata.close()\n",
    "    vdata.close()\n",
    "    \n",
    "    return var\n",
    "    \n",
    "#def read_IMERG_CT(filename)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f218b9a-76cb-4795-9c8f-ff8c811b2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in dataset from Bowen\n",
    "BP_path = '/home/pmarin/INCUS/Code/IMERG/IMERG_CT_BP/IMERG_CT_Analyses/BP_Data_2007/'\n",
    "BPfiles = glob.glob(BP_path+'*csv')\n",
    "\n",
    "BPdata = pd.DataFrame()\n",
    "for i in np.arange(0,len(BPfiles)):\n",
    "    BPcur = pd.read_csv(BPfiles[i])\n",
    "    BPdata = pd.concat([BPdata, BPcur],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93d4eba7-ef70-45c3-80ad-7354435d325e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JDate</th>\n",
       "      <th>Granuel</th>\n",
       "      <th>STID</th>\n",
       "      <th>UTC Time</th>\n",
       "      <th>Day or Night</th>\n",
       "      <th>MeanFlag</th>\n",
       "      <th>Pedestal_LON</th>\n",
       "      <th>Pedestal_LAT</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32</td>\n",
       "      <td>4060</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>-37.84</td>\n",
       "      <td>-47.32</td>\n",
       "      <td>-37.73</td>\n",
       "      <td>-47.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32</td>\n",
       "      <td>4060</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.11</td>\n",
       "      <td>146.94</td>\n",
       "      <td>-22.22</td>\n",
       "      <td>147.15</td>\n",
       "      <td>-23.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>4060</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.15</td>\n",
       "      <td>142.78</td>\n",
       "      <td>-3.51</td>\n",
       "      <td>142.78</td>\n",
       "      <td>-3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>4060</td>\n",
       "      <td>34</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>141.06</td>\n",
       "      <td>4.59</td>\n",
       "      <td>140.98</td>\n",
       "      <td>4.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>4061</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-50.81</td>\n",
       "      <td>-2.38</td>\n",
       "      <td>-50.71</td>\n",
       "      <td>-1.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20143</th>\n",
       "      <td>90</td>\n",
       "      <td>4904</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>156.45</td>\n",
       "      <td>4.76</td>\n",
       "      <td>156.45</td>\n",
       "      <td>4.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20144</th>\n",
       "      <td>90</td>\n",
       "      <td>4904</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>155.44</td>\n",
       "      <td>9.48</td>\n",
       "      <td>155.54</td>\n",
       "      <td>9.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20145</th>\n",
       "      <td>90</td>\n",
       "      <td>4905</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1.19</td>\n",
       "      <td>133.11</td>\n",
       "      <td>-1.72</td>\n",
       "      <td>133.13</td>\n",
       "      <td>-1.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20146</th>\n",
       "      <td>90</td>\n",
       "      <td>4905</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.00</td>\n",
       "      <td>132.60</td>\n",
       "      <td>0.66</td>\n",
       "      <td>132.65</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20147</th>\n",
       "      <td>90</td>\n",
       "      <td>4905</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1.33</td>\n",
       "      <td>-58.96</td>\n",
       "      <td>3.02</td>\n",
       "      <td>-58.83</td>\n",
       "      <td>3.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20148 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       JDate   Granuel    STID   UTC Time  Day or Night  MeanFlag  \\\n",
       "0         32       4060       1         4             0      2.00   \n",
       "1         32       4060      12         4             1      1.11   \n",
       "2         32       4060      25         4             1      1.15   \n",
       "3         32       4060      34         4             1      2.00   \n",
       "4         32       4061       0         5             0      1.74   \n",
       "...      ...        ...     ...       ...           ...       ...   \n",
       "20143     90       4904      28         3             1      2.00   \n",
       "20144     90       4904      43         3             1      2.00   \n",
       "20145     90       4905      19         5             1      1.19   \n",
       "20146     90       4905      19         5             1      2.00   \n",
       "20147     90       4905      32         6             0      1.33   \n",
       "\n",
       "       Pedestal_LON  Pedestal_LAT  Longitude  Latitude  \n",
       "0            -37.84        -47.32     -37.73    -47.02  \n",
       "1            146.94        -22.22     147.15    -23.04  \n",
       "2            142.78         -3.51     142.78     -3.49  \n",
       "3            141.06          4.59     140.98      4.95  \n",
       "4            -50.81         -2.38     -50.71     -1.89  \n",
       "...             ...           ...        ...       ...  \n",
       "20143        156.45          4.76     156.45      4.74  \n",
       "20144        155.44          9.48     155.54      9.02  \n",
       "20145        133.11         -1.72     133.13     -1.83  \n",
       "20146        132.60          0.66     132.65      0.44  \n",
       "20147        -58.96          3.02     -58.83      3.60  \n",
       "\n",
       "[20148 rows x 10 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BPdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f171a428-6366-4c06-9741-fcd9511d7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific .mat files that has information of IMERG_CT tracks\n",
    "#matpaths = ['/home/pmarin/INCUS/Code/IMERG/IMERG_CT_20070112.mat',\n",
    "#            '/home/pmarin/INCUS/Code/IMERG/IMERG_CT_20080112.mat']\n",
    "#matpaths = ['/home/pmarin/INCUS/Code/IMERG/New_IMERG_CT_2007.mat']\n",
    "matpaths = ['/avalanche/pmarin/INCUS/IMERG-CT/IMERG_data2_with_HXG_MERIR_2007.nc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6696a-bc3f-452f-b355-05eb7ca83b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b863360-9bcd-49be-b241-eebe33e50079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in a list of the IMERG-CT Dataset Variables\n",
    "arrays = {}\n",
    "f = h5py.File(matpaths[i])\n",
    "for k, v in f.items():\n",
    "    arrays[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae9533eb-eda9-441d-9802-f809fd96368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy Specific Variables from IMERG-CT Dataset to read-in\n",
    "# Family / Convective System\n",
    "vars_in = ['CS_id','CS_year','CS_month','CS_day','CS_hour','CS_life','frac_ocean',\n",
    "           'max_lat','min_lat','max_lon','min_lon','minCTT_MERGED','CS_minminCTT_MERGED','maxP','CS_maxmaxp','area','area_low']\n",
    "# Family \n",
    "# area_low (total area of system based on precitation rate of > 0.5 mm/hr)\n",
    "# area (convective system area precipitation 5 mm/hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "353c1620-409c-4532-8141-aac6a8e8b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in specific variables (vars_in) from the IMERG-CT and convert to pandas DF\n",
    "data_in = OrderedDict()\n",
    "for v in np.arange(0,len(vars_in)):\n",
    "    data_in[vars_in[v]] = arrays[vars_in[v]][0]\n",
    "    \n",
    "df_CT = pd.DataFrame.from_dict(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e32b0-2f97-4f9a-8eaa-7f9f23c6c0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63fa6ab3-44e6-4168-9857-2b161068118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of unique identifiers\n",
    "uni_ids = np.unique(df_CT['CS_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a58c265-5fc2-4abc-b144-8fe57860b631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open file with xarray\n",
    "#cdata = xr.open_dataset(inpath+cpath+era5_datestr+'.nc')\n",
    "#cdata = xr.open_dataset(inpath+cpath+era5_datestr+'_'+hemi+'.nc')\n",
    "\n",
    "# Get time index (idt) within each file\n",
    "#diff_time = pd.to_datetime(cdata.time.values)-date_i\n",
    "#idt = np.where(diff_time == np.min(np.abs(diff_time)))\n",
    "\n",
    "# Get lon index within each file\n",
    "#diff_lon = cdata.longitude.values - loni\n",
    "#idlon = np.where(diff_lon == np.min(np.abs(diff_lon)))\n",
    "\n",
    "# Get lat index within each file\n",
    "#diff_lat = cdata.latitude.values - lati\n",
    "#idlat = np.where(diff_lat == np.min(np.abs(diff_lat)))\n",
    "\n",
    "#cdata.cape['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fab9bb95-195b-4a8b-856d-79193d1d58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpath = '/avalanche/pmarin/INCUS/ERA5/GEO/GEO_2007_12_SH.nc'\n",
    "#test = xr.open_dataset(inpath)\n",
    "#print(test)\n",
    "\n",
    "#inpath = '/avalanche/pmarin/INCUS/ERA5/FL/FL_2007_12.nc'\n",
    "#test = xr.open_dataset(inpath)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ed49c-9ba0-495c-9484-fcf2d558871c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21665824-08af-4fea-bda9-ef397269814e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d82b5e-79f8-4b40-a107-264107f93974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "55289cf8-9bd1-42d9-938f-167793ba3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an hourly list of dates for the entire year 2007\n",
    "numhrs = 8760\n",
    "base = datetime.datetime(2007,1,1,0)\n",
    "date_list = [base + datetime.timedelta(hours=x) for x in range(numhrs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbe4c9-75e5-4086-90dd-959ef2f4a7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05263373-e807-4336-83e9-3b9a5007c066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_937/3875488777.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_0['lati'] = lati\n",
      "/tmp/ipykernel_937/3875488777.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_0['loni'] = loni\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'date_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Current date and time of initial precipitation detection of IMERG-CT families\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rand_on \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     date_i \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[43mdate_list\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     date_i \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime(\u001b[38;5;28mint\u001b[39m(df_0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS_year\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mint\u001b[39m(df_0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS_month\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mint\u001b[39m(df_0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS_day\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mint\u001b[39m(df_0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS_hour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'date_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Pull colocated data from ERA5\n",
    "\n",
    "rand_on = 0 # if 1, choose random lat/lon/time \n",
    "saveadd = 'rand' #Additional save name\n",
    "\n",
    "inpath = '/avalanche/pmarin/INCUS/ERA5/' # ERA5 datapaths\n",
    "\n",
    "pickpath = '/avalanche/pmarin/INCUS/IMERG-CT/pickle/' # Save pickle file path\n",
    "\n",
    "# Lat Lon Thresholds\n",
    "lat_arr = [-20, 20]\n",
    "lon_arr = [-180, 180]\n",
    "\n",
    "len_thr = 3 # Atleast three time steps long (i.e., 1 hour)\n",
    "\n",
    "comb_data = OrderedDict()\n",
    "data_all = []\n",
    "for i in np.arange(0,1):\n",
    "    \n",
    "    # Filename to save\n",
    "    filename = 'IMERG-CT_ERA5_LAT'+str(lat_arr[0])+str(lat_arr[1])+'_LON'+str(lon_arr[0])+str(lon_arr[1])+saveadd+'.p'\n",
    "\n",
    "    # Loop through unique objects and find ERA5 variables\n",
    "    for n in np.arange(0,len(uni_ids),1):\n",
    "        print('n=',n)\n",
    "        #    for n in np.arange(0,2,1):\n",
    "        fam1 = uni_ids[n]\n",
    "        #print(fam1)\n",
    "        \n",
    "        # Grab dataframe associated with current family\n",
    "        cur_df = df_CT[df_CT['CS_id'] == uni_ids[n]]\n",
    "        df_0 = cur_df.head(1)\n",
    "        #print(n,df_0)\n",
    "        \n",
    "        # Threshold families that last at least len_thr time steps\n",
    "        if df_0['CS_life'].iloc[0] < len_thr:\n",
    "            continue\n",
    "\n",
    "        # Get average lat / lon at initial time from max and min lat and lon values\n",
    "        if rand_on == 1:\n",
    "            lati = random.choice(np.arange(lat_arr[0]*100,lat_arr[1]*100,5)/100)\n",
    "        else:\n",
    "            lati = (df_0['max_lat'].iloc[0] + df_0['min_lat'].iloc[0])/2.0\n",
    "        df_0['lati'] = lati\n",
    "        \n",
    "        if rand_on == 1:\n",
    "            loni = random.choice(np.arange(lon_arr[0]*100,lon_arr[1]*100,5)/100)\n",
    "        else:\n",
    "            loni = (df_0['max_lon'].iloc[0] + df_0['min_lon'].iloc[0])/2.0\n",
    "        df_0['loni'] = loni\n",
    "               \n",
    "        # Exclude events at latitudes greater than |35|\n",
    "        if np.abs(lati) > 35:\n",
    "            continue\n",
    "\n",
    "        # Define NH or SH for filenaming convention for ERA5 files\n",
    "        if lati >= 0:\n",
    "            hemi = 'NH'\n",
    "        else:\n",
    "            hemi = 'SH'  \n",
    "\n",
    "        # Current date and time of initial precipitation detection of IMERG-CT families\n",
    "        if rand_on == 1:\n",
    "            date_i = random.choice(date_list)\n",
    "        else:\n",
    "            date_i = datetime.datetime(int(df_0['CS_year'].iloc[0]),int(df_0['CS_month'].iloc[0]),int(df_0['CS_day'].iloc[0]),int(df_0['CS_hour'].iloc[0]))\n",
    "        \n",
    "        # Look at certain time (i.e., 2 hours before detection)\n",
    "        date_i = date_i+datetime.timedelta(hours=0)\n",
    "        if date_i.year == 2006: # Only 2007/2008 data\n",
    "            continue\n",
    "#        print(date_i)\n",
    "\n",
    "\n",
    "        # Format for finding the correct era5 file\n",
    "        era5_datestr = date_i.strftime(\"%Y_%m\")   \n",
    "\n",
    "        # Low Level Wind Shear\n",
    "        alt_arr = [800, 1000]\n",
    "        upath = inpath+'U/U_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        vpath = inpath+'V/V_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_layer_shear(alt_arr,upath,vpath,lati,loni,date_i)\n",
    "        df_0['llws'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,upath,vpath)\n",
    "        \n",
    "        # Mid Level Wind Shear\n",
    "        alt_arr = [400, 800]\n",
    "        upath = inpath+'U/U_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        vpath = inpath+'V/V_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_layer_shear(alt_arr,upath,vpath,lati,loni,date_i)\n",
    "        df_0['mlws'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,upath,vpath)\n",
    "\n",
    "        # Low Level Specific Humidity\n",
    "        alt_arr = [800, 1000]\n",
    "        path = inpath+'SH/SH_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_layer_sh(alt_arr,path,lati,loni,date_i)\n",
    "        #print(var0)\n",
    "        df_0['llsh'] = copy.deepcopy(np.round(var0*1000,4))\n",
    "        del(var0,alt_arr,path)      \n",
    "        \n",
    "        # Mid Level Specific Humidity\n",
    "        alt_arr = [400, 800]\n",
    "        path = inpath+'SH/SH_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_layer_sh(alt_arr,path,lati,loni,date_i)\n",
    "        df_0['mlsh'] = copy.deepcopy(np.round(var0*1000,4))\n",
    "        del(var0,alt_arr,path)      \n",
    "               \n",
    "        # Mid Level Lapse Rate\n",
    "        alt_arr = [400, 800]\n",
    "        t_path = inpath+'T/T_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        z_path = inpath+'GEO/GEO_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_lapse_rate(alt_arr,t_path,z_path,lati,loni,date_i)\n",
    "        df_0['mllr'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,t_path,z_path)      \n",
    "\n",
    "        # MUCAPE\n",
    "        path = inpath+'MUCAPE/MUCAPE_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['mucape'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "\n",
    "        # CIN\n",
    "        path = inpath+'CIN/CIN_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['mlcin'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "\n",
    "        # FL\n",
    "        path = inpath+'FL/FL_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['fl'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "                \n",
    "        #print('DFO')\n",
    "        #print(df_0)\n",
    "        #print('_________________________________')\n",
    "        if n == 0:\n",
    "            data_all = copy.deepcopy(df_0)\n",
    "        else:\n",
    "            data_all = pd.concat([data_all,df_0])\n",
    "\n",
    "file = open(pickpath+filename, 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(data_all, file)\n",
    "file.close() # close the file            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d696d0-fc43-4925-a1e7-80179881f244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63957266-5086-4d43-8f72-f5c90b12af9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a44e4-fdeb-407c-b7a3-94ad20421967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e0a24-6934-4f1d-93f1-0a61fa8281a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8fd7b-8fa7-4bc2-965e-bf1a4fc076f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b975818-4dfe-47da-9166-4eb4ec9bec01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28479586-503c-47fb-a73d-97b6cc927d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea363c-49ce-4c4f-8dfb-1b37669e0052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c7ad9-b4fe-4650-82b8-0eae2ad6fadc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40b701-947c-49ce-a12b-74e06242f8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefd56a-7662-4128-a72d-6e4dc6b17f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efd166-b986-49d3-ad37-c166c8cc192c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7d73b-beda-4ea3-8b6e-3357e208840b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e4650-b51c-4a1f-be7b-bcb04e80d3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965e4ef-255f-4b7a-8a34-c2e09b88f192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26edb0-81c6-4430-8e2a-69d1f9c6170c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5ea50-223d-40ab-9b61-ee8085a16f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9f835-bb48-4ad5-8fa8-5a1a1df343d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b878db3-5ff7-4aad-a435-80aed59fbb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682d952-55b3-4773-98d2-087532f3e2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20029af-860f-4a5e-9c0a-289ec1b7a79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49580e51-1b72-4276-bb82-7db9ad3bf830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
