{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0434a0c9-c136-4a83-aea8-3e68feb8117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2880c411-d7d1-441d-ab78-6b51ac0a9afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ERA5 calculation functions\n",
    "\n",
    "# Function to get 2D variable data\n",
    "def calc_2D_var(data_path,lati,loni,datei):\n",
    "\n",
    "    data = xr.open_dataset(data_path)\n",
    "\n",
    "    varname = list(data.keys())[0]\n",
    "    data = data[varname]\n",
    "    \n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(data.time.values)-date_i\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))\n",
    "\n",
    "    # Get lon index within each file\n",
    "    diff_lon = data.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = data.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    var = data[int(idt[0]),int(idlat[0]),int(idlon[0])].values\n",
    "    \n",
    "    return var\n",
    "            \n",
    "# Function to calculate the mean specific humidity between two pressure levels\n",
    "def calc_mean_layer_sh(alt_arr,data_path,lati,loni,datei):\n",
    "\n",
    "    # Open up data with xarray\n",
    "    data = xr.open_dataset(data_path)\n",
    "\n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(data.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))\n",
    "            \n",
    "    # Get lon index within each file\n",
    "    diff_lon = data.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = data.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    # Get pressure level indicies\n",
    "    alt0 = np.where(np.abs(np.array(data.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(data.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(data.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(data.level[:].values) - alt_arr[1])))\n",
    "    #if n == 0:\n",
    "    #    print(alt0[0],alt1[0])\n",
    "\n",
    "    # Get specific data and do calculation\n",
    "    var1 = data.q[int(idt[0]),int(alt0[0]):int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var = np.nanmean(var1)\n",
    "    del(var1)\n",
    "\n",
    "    data.close()\n",
    "\n",
    "    return var\n",
    "        \n",
    "# Function to calculate the mean lapse rate between two pressure levels ( K/km )\n",
    "def calc_mean_lapse_rate(alt_arr,t_data_path,z_data_path,lati,loni,datei):\n",
    "\n",
    "    g = 9.807 #m/s^2\n",
    "\n",
    "    # Open up data with xarray\n",
    "    tdata = xr.open_dataset(t_data_path)\n",
    "    zdata = xr.open_dataset(z_data_path)\n",
    "\n",
    "    \n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(tdata.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))    \n",
    "    \n",
    "    # Get lon index within each file\n",
    "    diff_lon = tdata.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = tdata.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    # Get pressure level indicies\n",
    "    alt0 = np.where(np.abs(np.array(tdata.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(tdata.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(tdata.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(tdata.level[:].values) - alt_arr[1])))\n",
    "    #if n == 0:\n",
    "    #    print(alt0[0],alt1[0])\n",
    "\n",
    "    # Get specific data and do calculation\n",
    "    var1 = tdata.t[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var2 = tdata.t[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "\n",
    "    var3 = zdata.z[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var4 = zdata.z[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "\n",
    "    var = (var1-var2)/((var3-var4)/g/1000)\n",
    "    del(var1,var2,var3,var4)\n",
    "\n",
    "    tdata.close()\n",
    "    zdata.close()\n",
    "\n",
    "    return var\n",
    "    \n",
    "\n",
    "# Function to calculate the wind shear between two pressure levels\n",
    "def calc_layer_shear(alt_arr,data_path_u,data_path_v,lati,loni,datei):\n",
    "\n",
    "    #### LOW LEVEL SHEAR        \n",
    "#    udata = xr.open_dataset(inpath+upath+era5_datestr+'_'+hemi+'.nc')\n",
    "#    vdata = xr.open_dataset(inpath+vpath+era5_datestr+'_'+hemi+'.nc')\n",
    "    udata = xr.open_dataset(data_path_u)\n",
    "    vdata = xr.open_dataset(data_path_v)\n",
    "\n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(udata.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))    \n",
    "\n",
    "    # Get lon index within each file\n",
    "    diff_lon = udata.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = vdata.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))   \n",
    "    \n",
    "#    alt_arr = [800, 1000]\n",
    "    alt0 = np.where(np.abs(np.array(udata.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(udata.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(udata.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(udata.level[:].values) - alt_arr[1])))\n",
    "    if n == 0:\n",
    "        print(alt0[0],alt1[0])\n",
    "\n",
    "    var1 = udata.u[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var2 = vdata.v[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var3 = udata.u[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var4 = vdata.v[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var = np.sqrt(np.power((var3-var1),2.0) + np.power((var4-var2),2.0))\n",
    "    #lls_vals = np.append(lls_vals,var)       \n",
    "    \n",
    "    del(var1,var2,var3,var4)\n",
    "    udata.close()\n",
    "    vdata.close()\n",
    "    \n",
    "    return var\n",
    "    \n",
    "#def read_IMERG_CT(filename)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f218b9a-76cb-4795-9c8f-ff8c811b2938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617363a-fdcb-47f6-8b81-afff5f3c2c80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff78455-242a-4980-8f0f-07c8fa9d38f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f171a428-6366-4c06-9741-fcd9511d7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific .mat files that has information of IMERG_CT tracks\n",
    "#matpaths = ['/home/pmarin/INCUS/Code/IMERG/IMERG_CT_20070112.mat',\n",
    "#            '/home/pmarin/INCUS/Code/IMERG/IMERG_CT_20080112.mat']\n",
    "#matpaths = ['/home/pmarin/INCUS/Code/IMERG/New_IMERG_CT_2007.mat']\n",
    "matpaths = ['/avalanche/pmarin/INCUS/IMERG-CT/IMERG_data2_with_HXG_MERIR_2007.nc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6696a-bc3f-452f-b355-05eb7ca83b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b863360-9bcd-49be-b241-eebe33e50079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "arrays = {}\n",
    "f = h5py.File(matpaths[i])\n",
    "for k, v in f.items():\n",
    "    arrays[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae9533eb-eda9-441d-9802-f809fd96368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Family / Convective System\n",
    "vars_in = ['CS_id','CS_year','CS_month','CS_day','CS_hour','CS_life','frac_ocean',\n",
    "           'max_lat','min_lat','max_lon','min_lon','minCTT_MERGED','CS_minminCTT_MERGED','maxP','CS_maxmaxp','area','area_low']\n",
    "\n",
    "# Family \n",
    "# area_low (total area of system based on precitation rate of > 0.5 mm/hr)\n",
    "# area (convective system area precipitation 5 mm/hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "353c1620-409c-4532-8141-aac6a8e8b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_in = OrderedDict()\n",
    "for v in np.arange(0,len(vars_in)):\n",
    "    data_in[vars_in[v]] = arrays[vars_in[v]][0]\n",
    "    \n",
    "df_CT = pd.DataFrame.from_dict(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e32b0-2f97-4f9a-8eaa-7f9f23c6c0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63fa6ab3-44e6-4168-9857-2b161068118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of unique identifiers\n",
    "uni_ids = np.unique(df_CT['CS_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7a58c265-5fc2-4abc-b144-8fe57860b631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open file with xarray\n",
    "#cdata = xr.open_dataset(inpath+cpath+era5_datestr+'.nc')\n",
    "#cdata = xr.open_dataset(inpath+cpath+era5_datestr+'_'+hemi+'.nc')\n",
    "\n",
    "# Get time index (idt) within each file\n",
    "#diff_time = pd.to_datetime(cdata.time.values)-date_i\n",
    "#idt = np.where(diff_time == np.min(np.abs(diff_time)))\n",
    "\n",
    "# Get lon index within each file\n",
    "#diff_lon = cdata.longitude.values - loni\n",
    "#idlon = np.where(diff_lon == np.min(np.abs(diff_lon)))\n",
    "\n",
    "# Get lat index within each file\n",
    "#diff_lat = cdata.latitude.values - lati\n",
    "#idlat = np.where(diff_lat == np.min(np.abs(diff_lat)))\n",
    "\n",
    "#cdata.cape['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab9bb95-195b-4a8b-856d-79193d1d58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpath = '/avalanche/pmarin/INCUS/ERA5/GEO/GEO_2007_12_SH.nc'\n",
    "#test = xr.open_dataset(inpath)\n",
    "#print(test)\n",
    "\n",
    "#inpath = '/avalanche/pmarin/INCUS/ERA5/FL/FL_2007_12.nc'\n",
    "#test = xr.open_dataset(inpath)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ed49c-9ba0-495c-9484-fcf2d558871c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21665824-08af-4fea-bda9-ef397269814e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d82b5e-79f8-4b40-a107-264107f93974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "55289cf8-9bd1-42d9-938f-167793ba3d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an hourly list of dates for the entire year 2007\n",
    "numhrs = 8760\n",
    "base = datetime.datetime(2007,1,1,0)\n",
    "date_list = [base + datetime.timedelta(hours=x) for x in range(numhrs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbe4c9-75e5-4086-90dd-959ef2f4a7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "05263373-e807-4336-83e9-3b9a5007c066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n= 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_937/3875488777.py:42: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_0['lati'] = lati\n",
      "/tmp/ipykernel_937/3875488777.py:48: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_0['loni'] = loni\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'date_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# Current date and time of initial precipitation detection of IMERG-CT families\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rand_on \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 62\u001b[0m     date_i \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(\u001b[43mdate_list\u001b[49m)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     date_i \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime(\u001b[38;5;28mint\u001b[39m(df_0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS_year\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mint\u001b[39m(df_0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS_month\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mint\u001b[39m(df_0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS_day\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]),\u001b[38;5;28mint\u001b[39m(df_0[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCS_hour\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'date_list' is not defined"
     ]
    }
   ],
   "source": [
    "# Define ERA5 filepaths for input and output, as well as specific files for globbing\n",
    "\n",
    "rand_on = 1\n",
    "inpath = '/avalanche/pmarin/INCUS/ERA5/'\n",
    "\n",
    "pickpath = '/avalanche/pmarin/INCUS/IMERG-CT/pickle/'\n",
    "\n",
    "# Lat Lon Thresholds\n",
    "lat_arr = [-20, 20]\n",
    "lon_arr = [-180, 180]\n",
    "\n",
    "len_thr = 3 # Atleast three time steps long (i.e., 1 hour)\n",
    "\n",
    "saveadd = 'rand'\n",
    "\n",
    "comb_data = OrderedDict()\n",
    "data_all = []\n",
    "for i in np.arange(0,1):\n",
    "    \n",
    "    filename = 'IMERG-CT_ERA5_LAT'+str(lat_arr[0])+str(lat_arr[1])+'_LON'+str(lon_arr[0])+str(lon_arr[1])+saveadd+'.p'\n",
    "\n",
    "    for n in np.arange(0,len(uni_ids),1):\n",
    "        print('n=',n)\n",
    "        #    for n in np.arange(0,2,1):\n",
    "        fam1 = uni_ids[n]\n",
    "        #print(fam1)\n",
    "        \n",
    "        # Grab dataframe associated with current family\n",
    "        cur_df = df_CT[df_CT['CS_id'] == uni_ids[n]]\n",
    "        df_0 = cur_df.head(1)\n",
    "        #print(n,df_0)\n",
    "        \n",
    "        # Threshold families that last at least len_thr time steps\n",
    "        if df_0['CS_life'].iloc[0] < len_thr:\n",
    "            continue\n",
    "\n",
    "        # Get average lat / lon at initial time from max and min lat and lon values\n",
    "        if rand_on == 1:\n",
    "            lati = random.choice(np.arange(lat_arr[0]*100,lat_arr[1]*100,5)/100)\n",
    "        else:\n",
    "            lati = (df_0['max_lat'].iloc[0] + df_0['min_lat'].iloc[0])/2.0\n",
    "        df_0['lati'] = lati\n",
    "        \n",
    "        if rand_on == 1:\n",
    "            loni = random.choice(np.arange(lon_arr[0]*100,lon_arr[1]*100,5)/100)\n",
    "        else:\n",
    "            loni = (df_0['max_lon'].iloc[0] + df_0['min_lon'].iloc[0])/2.0\n",
    "        df_0['loni'] = loni\n",
    "               \n",
    "        # Exclude events at latitudes greater than |35|\n",
    "        if np.abs(lati) > 35:\n",
    "            continue\n",
    "\n",
    "        # Define NH or SH for filenaming convention for ERA5 files\n",
    "        if lati >= 0:\n",
    "            hemi = 'NH'\n",
    "        else:\n",
    "            hemi = 'SH'  \n",
    "\n",
    "        # Current date and time of initial precipitation detection of IMERG-CT families\n",
    "        if rand_on == 1:\n",
    "            date_i = random.choice(date_list)\n",
    "        else:\n",
    "            date_i = datetime.datetime(int(df_0['CS_year'].iloc[0]),int(df_0['CS_month'].iloc[0]),int(df_0['CS_day'].iloc[0]),int(df_0['CS_hour'].iloc[0]))\n",
    "        \n",
    "        # Look at certain time (i.e., 2 hours before detection)\n",
    "        date_i = date_i+datetime.timedelta(hours=0)\n",
    "        if date_i.year == 2006: # Only 2007/2008 data\n",
    "            continue\n",
    "#        print(date_i)\n",
    "\n",
    "\n",
    "        # Format for finding the correct era5 file\n",
    "        era5_datestr = date_i.strftime(\"%Y_%m\")   \n",
    "\n",
    "        # Low Level Wind Shear\n",
    "        alt_arr = [800, 1000]\n",
    "        upath = inpath+'U/U_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        vpath = inpath+'V/V_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_layer_shear(alt_arr,upath,vpath,lati,loni,date_i)\n",
    "        df_0['llws'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,upath,vpath)\n",
    "        \n",
    "        # Mid Level Wind Shear\n",
    "        alt_arr = [400, 800]\n",
    "        upath = inpath+'U/U_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        vpath = inpath+'V/V_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_layer_shear(alt_arr,upath,vpath,lati,loni,date_i)\n",
    "        df_0['mlws'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,upath,vpath)\n",
    "\n",
    "        # Low Level Specific Humidity\n",
    "        alt_arr = [800, 1000]\n",
    "        path = inpath+'SH/SH_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_layer_sh(alt_arr,path,lati,loni,date_i)\n",
    "        #print(var0)\n",
    "        df_0['llsh'] = copy.deepcopy(np.round(var0*1000,4))\n",
    "        del(var0,alt_arr,path)      \n",
    "        \n",
    "        # Mid Level Specific Humidity\n",
    "        alt_arr = [400, 800]\n",
    "        path = inpath+'SH/SH_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_layer_sh(alt_arr,path,lati,loni,date_i)\n",
    "        df_0['mlsh'] = copy.deepcopy(np.round(var0*1000,4))\n",
    "        del(var0,alt_arr,path)      \n",
    "               \n",
    "        # Mid Level Lapse Rate\n",
    "        alt_arr = [400, 800]\n",
    "        t_path = inpath+'T/T_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        z_path = inpath+'GEO/GEO_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_lapse_rate(alt_arr,t_path,z_path,lati,loni,date_i)\n",
    "        df_0['mllr'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,t_path,z_path)      \n",
    "\n",
    "        # MUCAPE\n",
    "        path = inpath+'MUCAPE/MUCAPE_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['mucape'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "\n",
    "        # CIN\n",
    "        path = inpath+'CIN/CIN_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['mlcin'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "\n",
    "        # FL\n",
    "        path = inpath+'FL/FL_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['fl'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "                \n",
    "        #print('DFO')\n",
    "        #print(df_0)\n",
    "        #print('_________________________________')\n",
    "        if n == 0:\n",
    "            data_all = copy.deepcopy(df_0)\n",
    "        else:\n",
    "            data_all = pd.concat([data_all,df_0])\n",
    "\n",
    "file = open(pickpath+filename, 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(data_all, file)\n",
    "file.close() # close the file            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d696d0-fc43-4925-a1e7-80179881f244",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63957266-5086-4d43-8f72-f5c90b12af9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a44e4-fdeb-407c-b7a3-94ad20421967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e0a24-6934-4f1d-93f1-0a61fa8281a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8fd7b-8fa7-4bc2-965e-bf1a4fc076f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b975818-4dfe-47da-9166-4eb4ec9bec01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28479586-503c-47fb-a73d-97b6cc927d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea363c-49ce-4c4f-8dfb-1b37669e0052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c7ad9-b4fe-4650-82b8-0eae2ad6fadc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40b701-947c-49ce-a12b-74e06242f8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefd56a-7662-4128-a72d-6e4dc6b17f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efd166-b986-49d3-ad37-c166c8cc192c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7d73b-beda-4ea3-8b6e-3357e208840b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e4650-b51c-4a1f-be7b-bcb04e80d3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965e4ef-255f-4b7a-8a34-c2e09b88f192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26edb0-81c6-4430-8e2a-69d1f9c6170c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5ea50-223d-40ab-9b61-ee8085a16f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9f835-bb48-4ad5-8fa8-5a1a1df343d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b878db3-5ff7-4aad-a435-80aed59fbb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682d952-55b3-4773-98d2-087532f3e2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20029af-860f-4a5e-9c0a-289ec1b7a79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49580e51-1b72-4276-bb82-7db9ad3bf830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
