{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0434a0c9-c136-4a83-aea8-3e68feb8117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Python Libraries\n",
    "import pickle\n",
    "import h5py\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import OrderedDict\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2880c411-d7d1-441d-ab78-6b51ac0a9afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ERA5 calculation functions\n",
    "\n",
    "# Function to get 2D variable data\n",
    "def calc_2D_var(data_path,lati,loni,datei):\n",
    "\n",
    "    data = xr.open_dataset(data_path)\n",
    "\n",
    "    varname = list(data.keys())[0]\n",
    "    data = data[varname]\n",
    "    \n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(data.time.values)-date_i\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))\n",
    "\n",
    "    # Get lon index within each file\n",
    "    diff_lon = data.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = data.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    var = data[int(idt[0]),int(idlat[0]),int(idlon[0])].values\n",
    "    \n",
    "    return var\n",
    "            \n",
    "# Function to calculate the mean specific humidity between two pressure levels\n",
    "def calc_mean_layer_sh(alt_arr,data_path,lati,loni,datei):\n",
    "\n",
    "    # Open up data with xarray\n",
    "    data = xr.open_dataset(data_path)\n",
    "\n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(data.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))\n",
    "            \n",
    "    # Get lon index within each file\n",
    "    diff_lon = data.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = data.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    # Get pressure level indicies\n",
    "    alt0 = np.where(np.abs(np.array(data.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(data.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(data.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(data.level[:].values) - alt_arr[1])))\n",
    "    #if n == 0:\n",
    "    #    print(alt0[0],alt1[0])\n",
    "\n",
    "    # Get specific data and do calculation\n",
    "    var1 = data.q[int(idt[0]),int(alt0[0]):int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var = np.nanmean(var1)\n",
    "    del(var1)\n",
    "\n",
    "    data.close()\n",
    "\n",
    "    return var\n",
    "        \n",
    "# Function to calculate the mean lapse rate between two pressure levels ( K/km )\n",
    "def calc_mean_lapse_rate(alt_arr,t_data_path,z_data_path,lati,loni,datei):\n",
    "\n",
    "    g = 9.807 #m/s^2\n",
    "\n",
    "    # Open up data with xarray\n",
    "    tdata = xr.open_dataset(t_data_path)\n",
    "    zdata = xr.open_dataset(z_data_path)\n",
    "\n",
    "    \n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(tdata.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))    \n",
    "    \n",
    "    # Get lon index within each file\n",
    "    diff_lon = tdata.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = tdata.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))\n",
    "\n",
    "    # Get pressure level indicies\n",
    "    alt0 = np.where(np.abs(np.array(tdata.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(tdata.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(tdata.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(tdata.level[:].values) - alt_arr[1])))\n",
    "    #if n == 0:\n",
    "    #    print(alt0[0],alt1[0])\n",
    "\n",
    "    # Get specific data and do calculation\n",
    "    var1 = tdata.t[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var2 = tdata.t[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "\n",
    "    var3 = zdata.z[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var4 = zdata.z[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "\n",
    "    var = (var1-var2)/((var3-var4)/g/1000)\n",
    "    del(var1,var2,var3,var4)\n",
    "\n",
    "    tdata.close()\n",
    "    zdata.close()\n",
    "\n",
    "    return var\n",
    "    \n",
    "\n",
    "# Function to calculate the wind shear between two pressure levels\n",
    "def calc_layer_shear(alt_arr,data_path_u,data_path_v,lati,loni,datei):\n",
    "\n",
    "    #### LOW LEVEL SHEAR        \n",
    "#    udata = xr.open_dataset(inpath+upath+era5_datestr+'_'+hemi+'.nc')\n",
    "#    vdata = xr.open_dataset(inpath+vpath+era5_datestr+'_'+hemi+'.nc')\n",
    "    udata = xr.open_dataset(data_path_u)\n",
    "    vdata = xr.open_dataset(data_path_v)\n",
    "\n",
    "    # Get time index (idt) within each file\n",
    "    diff_time = pd.to_datetime(udata.time.values)-datei\n",
    "    idt = np.where(np.abs(diff_time) == np.min(np.abs(diff_time)))    \n",
    "\n",
    "    # Get lon index within each file\n",
    "    diff_lon = udata.longitude.values - loni\n",
    "    idlon = np.where(np.abs(diff_lon) == np.min(np.abs(diff_lon)))\n",
    "\n",
    "    # Get lat index within each file\n",
    "    diff_lat = vdata.latitude.values - lati\n",
    "    idlat = np.where(np.abs(diff_lat) == np.min(np.abs(diff_lat)))   \n",
    "    \n",
    "#    alt_arr = [800, 1000]\n",
    "    alt0 = np.where(np.abs(np.array(udata.level[:].values) - alt_arr[0]) == np.min(np.abs(np.array(udata.level[:].values) - alt_arr[0])))\n",
    "    alt1 = np.where(np.abs(np.array(udata.level[:].values) - alt_arr[1]) == np.min(np.abs(np.array(udata.level[:].values) - alt_arr[1])))\n",
    "\n",
    "    #if n == 0:\n",
    "    #    print(alt0[0],alt1[0])\n",
    "\n",
    "    var1 = udata.u[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var2 = vdata.v[int(idt[0]),int(alt0[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var3 = udata.u[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var4 = vdata.v[int(idt[0]),int(alt1[0]),int(idlat[0]),int(idlon[0])]\n",
    "    var = np.sqrt(np.power((var3-var1),2.0) + np.power((var4-var2),2.0))\n",
    "    #lls_vals = np.append(lls_vals,var)       \n",
    "    \n",
    "    del(var1,var2,var3,var4)\n",
    "    udata.close()\n",
    "    vdata.close()\n",
    "    \n",
    "    return var\n",
    "    \n",
    "#def read_IMERG_CT(filename)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f218b9a-76cb-4795-9c8f-ff8c811b2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in dataset from Bowen\n",
    "BP_path = '/home/pmarin/INCUS/Code/IMERG/IMERG_CT_BP/IMERG_CT_Analyses/BP_Data_2007/'\n",
    "BPfiles = glob.glob(BP_path+'*csv')\n",
    "\n",
    "BPdata = pd.DataFrame()\n",
    "for i in np.arange(0,len(BPfiles)):\n",
    "    BPcur = pd.read_csv(BPfiles[i])\n",
    "    BPdata = pd.concat([BPdata, BPcur],ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d4eba7-ef70-45c3-80ad-7354435d325e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f171a428-6366-4c06-9741-fcd9511d7d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific .mat files that has information of IMERG_CT tracks\n",
    "#matpaths = ['/home/pmarin/INCUS/Code/IMERG/IMERG_CT_20070112.mat',\n",
    "#            '/home/pmarin/INCUS/Code/IMERG/IMERG_CT_20080112.mat']\n",
    "#matpaths = ['/home/pmarin/INCUS/Code/IMERG/New_IMERG_CT_2007.mat']\n",
    "matpaths = ['/avalanche/pmarin/INCUS/IMERG-CT/IMERG_data2_with_HXG_MERIR_2007.nc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd6696a-bc3f-452f-b355-05eb7ca83b09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b863360-9bcd-49be-b241-eebe33e50079",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read in a list of the IMERG-CT Dataset Variables\n",
    "i = 0\n",
    "arrays = {}\n",
    "f = h5py.File(matpaths[i])\n",
    "for k, v in f.items():\n",
    "    arrays[k] = np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae9533eb-eda9-441d-9802-f809fd96368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifiy Specific Variables from IMERG-CT Dataset to read-in\n",
    "# Family / Convective System\n",
    "vars_in = ['CS_id','CS_year','CS_month','CS_day','CS_hour','CS_life','frac_ocean',\n",
    "           'max_lat','min_lat','max_lon','min_lon','minCTT_MERGED','CS_minminCTT_MERGED','maxP','CS_maxmaxp','area','area_low']\n",
    "# Family \n",
    "# area_low (total area of system based on precitation rate of > 0.5 mm/hr)\n",
    "# area (convective system area precipitation 5 mm/hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "353c1620-409c-4532-8141-aac6a8e8b826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in specific variables (vars_in) from the IMERG-CT and convert to pandas DF\n",
    "data_in = OrderedDict()\n",
    "for v in np.arange(0,len(vars_in)):\n",
    "    data_in[vars_in[v]] = arrays[vars_in[v]][0]\n",
    "    \n",
    "df_CT = pd.DataFrame.from_dict(data_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530e32b0-2f97-4f9a-8eaa-7f9f23c6c0ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63fa6ab3-44e6-4168-9857-2b161068118e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get List of unique identifiers\n",
    "uni_ids = np.unique(df_CT['CS_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a58c265-5fc2-4abc-b144-8fe57860b631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open file with xarray\n",
    "#cdata = xr.open_dataset(inpath+cpath+era5_datestr+'.nc')\n",
    "#cdata = xr.open_dataset(inpath+cpath+era5_datestr+'_'+hemi+'.nc')\n",
    "\n",
    "# Get time index (idt) within each file\n",
    "#diff_time = pd.to_datetime(cdata.time.values)-date_i\n",
    "#idt = np.where(diff_time == np.min(np.abs(diff_time)))\n",
    "\n",
    "# Get lon index within each file\n",
    "#diff_lon = cdata.longitude.values - loni\n",
    "#idlon = np.where(diff_lon == np.min(np.abs(diff_lon)))\n",
    "\n",
    "# Get lat index within each file\n",
    "#diff_lat = cdata.latitude.values - lati\n",
    "#idlat = np.where(diff_lat == np.min(np.abs(diff_lat)))\n",
    "\n",
    "#cdata.cape['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab9bb95-195b-4a8b-856d-79193d1d58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inpath = '/avalanche/pmarin/INCUS/ERA5/GEO/GEO_2007_12_SH.nc'\n",
    "#test = xr.open_dataset(inpath)\n",
    "#print(test)\n",
    "\n",
    "#inpath = '/avalanche/pmarin/INCUS/ERA5/FL/FL_2007_12.nc'\n",
    "#test = xr.open_dataset(inpath)\n",
    "#print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0ed49c-9ba0-495c-9484-fcf2d558871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an hourly list of dates for the entire year 2007\n",
    "#numhrs = 8760\n",
    "#base = datetime.datetime(2007,1,1,0)\n",
    "#date_list = [base + datetime.timedelta(hours=x) for x in range(numhrs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21665824-08af-4fea-bda9-ef397269814e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d82b5e-79f8-4b40-a107-264107f93974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55289cf8-9bd1-42d9-938f-167793ba3d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbe4c9-75e5-4086-90dd-959ef2f4a7c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded9595a-c59a-4e21-b392-70ebca7efed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ae7893-117a-4d6a-afcb-0d5ba25ce335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474ab379-7532-49bb-b717-a567eff8a8e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496a1262-41ca-4fa8-8c33-6fad77ec31cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cc9cfc-4b1e-4b2e-8be1-d87d9f131cac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f31cf5a-6403-4681-b011-51246a7c6cee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899eeeb-3420-43d1-8774-566b8b7bf992",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05263373-e807-4336-83e9-3b9a5007c066",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdfsdf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pull colocated data from ERA5\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43msdfsdf\u001b[49m\n\u001b[1;32m      3\u001b[0m rand_on \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;66;03m# if 1, choose random lat/lon/time \u001b[39;00m\n\u001b[1;32m      4\u001b[0m saveadd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBP_PSystem\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;66;03m#Additional save name\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdfsdf' is not defined"
     ]
    }
   ],
   "source": [
    "# Pull colocated data from ERA5\n",
    "\n",
    "rand_on = 0 # if 1, choose random lat/lon/time \n",
    "saveadd = 'BP' #Additional save name\n",
    "\n",
    "inpath = '/avalanche/pmarin/INCUS/ERA5/' # ERA5 datapaths\n",
    "\n",
    "lat_arr = [-20,20]\n",
    "lon_arr = [-180,180]\n",
    "\n",
    "pickpath = '/avalanche/pmarin/INCUS/IMERG-CT/BP/pickle/' # Save pickle file path\n",
    "\n",
    "len_thr = 3 # Atleast three time steps long (i.e., 1 hour)\n",
    "\n",
    "comb_data = OrderedDict()\n",
    "data_all = []\n",
    "for i in np.arange(0,1):\n",
    "    \n",
    "    # Filename to save\n",
    "    filename = 'IMERG-CT_ERA5_LAT'+str(lat_arr[0])+str(lat_arr[1])+'_LON'+str(lon_arr[0])+str(lon_arr[1])+saveadd+'.p'\n",
    "\n",
    "    # Loop through Bowen's locations and times\n",
    "    cntm = 0\n",
    "    for m in np.arange(0,len(BPdata)):\n",
    "\n",
    "        print(m)\n",
    "        \n",
    "        BPcur = BPdata.iloc[m]\n",
    "        #print(BPcur)\n",
    "        ##\n",
    "        hr_bp = BPcur['UTC Time']\n",
    "        day_bp = BPcur['JDate']\n",
    "        lon_bp = BPcur['Longitude']\n",
    "        lat_bp = BPcur['Latitude']\n",
    "        don_bp = BPcur['Day or Night']\n",
    "        mf_bp = BPcur['MeanFlag']\n",
    "        id_bp = BPcur[' STID ']\n",
    "        gr_bp = BPcur[' Granuel ']\n",
    "        \n",
    "        # Convert Julian date to python datetime\n",
    "        date_bp = datetime.datetime.strptime('07'+str(int(day_bp)), '%y%j').date()\n",
    "        month_bp = date_bp.month \n",
    "        dd_bp = date_bp.day\n",
    "\n",
    "        #subset IMERGCT dataset based on date (month, day, and hour (within 2 hours)) \n",
    "        df_CT_sub = df_CT[df_CT['CS_month'] == month_bp]\n",
    "        df_CT_sub = df_CT_sub[df_CT_sub['CS_day'] == dd_bp]\n",
    "        df_CT_sub = df_CT_sub[np.abs(df_CT_sub['CS_hour'] - hr_bp) < 2]\n",
    "\n",
    "        #Further subset IMERGCT dataset based on being within the min/max lats and lons in the IMERG-CT Dataset\n",
    "        df_CT_sub1 = df_CT_sub[lat_bp >= df_CT_sub['min_lat']] \n",
    "        df_CT_sub1 = df_CT_sub1[lat_bp < df_CT_sub1['max_lat']] \n",
    "\n",
    "        df_CT_sub2 = df_CT_sub1[lon_bp >= df_CT_sub1['min_lon']] \n",
    "        df_CT_sub2 = df_CT_sub2[lon_bp < df_CT_sub2['max_lon']] \n",
    "\n",
    "        # If not overlap between Bowen's Cloudsat and IMERG-CT Datasets move onto next feature in Bowen's Dataset\n",
    "        if len(df_CT_sub2) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Determine how many unique items are part of this feature\n",
    "        uni_ids_sub = np.unique(df_CT_sub2['CS_id'])\n",
    "        \n",
    "        # Loop through systems and choose the systems with the shortest lifetime (Precipitation Cores)\n",
    "        for u in np.arange(0,len(uni_ids_sub)):\n",
    "            cur_uni_id = uni_ids_sub[u]\n",
    "            cur_uni_df = df_CT[df_CT['CS_id'] == cur_uni_id]\n",
    "            if u == 0:\n",
    "                life_fin = cur_uni_df.iloc[0]['CS_life']\n",
    "                uni_fin = cur_uni_id\n",
    "            elif cur_uni_df.iloc[0]['CS_life'] < life_fin:\n",
    "                uni_fin = cur_uni_id\n",
    "        \n",
    "        # Grab dataframe from original IMERG-CT dataframe associated with unique id\n",
    "        cur_df = df_CT[df_CT['CS_id'] == uni_fin]\n",
    "        df_0 = cur_df.head(1)\n",
    "        #print(df_0)\n",
    "\n",
    "        # Add Bowen's Cloudset Data to this Dataframe\n",
    "        df_0['BP_UTC_TIME'] = hr_bp\n",
    "        df_0['BP_JDATE'] = day_bp\n",
    "        df_0['BP_LON'] = lon_bp\n",
    "        df_0['BP_LAT'] = lat_bp\n",
    "        df_0['BP_DON'] = don_bp\n",
    "        df_0['BP_MEANFLAG'] = mf_bp\n",
    "        df_0['BP_STID'] = id_bp\n",
    "        df_0['BP_GRANUEL'] = gr_bp\n",
    "   \n",
    "        # Threshold families that last at least len_thr time steps\n",
    "        if df_0['CS_life'].iloc[0] < len_thr:\n",
    "            continue\n",
    "\n",
    "        # Get average lat / lon at initial time from max and min lat and lon values\n",
    "        if rand_on == 1:\n",
    "            lati = random.choice(np.arange(lat_arr[0]*100,lat_arr[1]*100,5)/100)\n",
    "        else:\n",
    "            lati = (df_0['max_lat'].iloc[0] + df_0['min_lat'].iloc[0])/2.0\n",
    "        df_0['lati'] = lati\n",
    "\n",
    "        if rand_on == 1:\n",
    "            loni = random.choice(np.arange(lon_arr[0]*100,lon_arr[1]*100,5)/100)\n",
    "        else:\n",
    "            loni = (df_0['max_lon'].iloc[0] + df_0['min_lon'].iloc[0])/2.0\n",
    "        df_0['loni'] = loni\n",
    "\n",
    "        # Exclude events at latitudes greater than |35|\n",
    "        if np.abs(lati) > 35:\n",
    "            continue\n",
    "\n",
    "        # Define NH or SH for filenaming convention for ERA5 files\n",
    "        if lati >= 0:\n",
    "            hemi = 'NH'\n",
    "        else:\n",
    "            hemi = 'SH'  \n",
    "\n",
    "        # Current date and time of initial precipitation detection of IMERG-CT families\n",
    "        if rand_on == 1:\n",
    "            date_i = random.choice(date_list)\n",
    "        else:\n",
    "            date_i = datetime.datetime(int(df_0['CS_year'].iloc[0]),int(df_0['CS_month'].iloc[0]),int(df_0['CS_day'].iloc[0]),int(df_0['CS_hour'].iloc[0]))\n",
    "\n",
    "        # Look at certain time (i.e., 2 hours before detection)\n",
    "        date_i = date_i+datetime.timedelta(hours=0)\n",
    "        if date_i.year == 2006: # Only 2007/2008 data\n",
    "            continue\n",
    "#        print(date_i)\n",
    "\n",
    "\n",
    "        # Format for finding the correct era5 file\n",
    "        era5_datestr = date_i.strftime(\"%Y_%m\")   \n",
    "\n",
    "        # Low Level Wind Shear\n",
    "        alt_arr = [800, 1000]\n",
    "        upath = inpath+'U/U_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        vpath = inpath+'V/V_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_layer_shear(alt_arr,upath,vpath,lati,loni,date_i)\n",
    "        df_0['llws'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,upath,vpath)\n",
    "\n",
    "        # Mid Level Wind Shear\n",
    "        alt_arr = [400, 800]\n",
    "        upath = inpath+'U/U_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        vpath = inpath+'V/V_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_layer_shear(alt_arr,upath,vpath,lati,loni,date_i)\n",
    "        df_0['mlws'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,upath,vpath)\n",
    "\n",
    "        # Low Level Specific Humidity\n",
    "        alt_arr = [800, 1000]\n",
    "        path = inpath+'SH/SH_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_layer_sh(alt_arr,path,lati,loni,date_i)\n",
    "        #print(var0)\n",
    "        df_0['llsh'] = copy.deepcopy(np.round(var0*1000,4))\n",
    "        del(var0,alt_arr,path)      \n",
    "\n",
    "        # Mid Level Specific Humidity\n",
    "        alt_arr = [400, 800]\n",
    "        path = inpath+'SH/SH_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_layer_sh(alt_arr,path,lati,loni,date_i)\n",
    "        df_0['mlsh'] = copy.deepcopy(np.round(var0*1000,4))\n",
    "        del(var0,alt_arr,path)      \n",
    "\n",
    "        # Mid Level Lapse Rate\n",
    "        alt_arr = [400, 800]\n",
    "        t_path = inpath+'T/T_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        z_path = inpath+'GEO/GEO_'+era5_datestr+'_'+hemi+'.nc'\n",
    "        var0 = calc_mean_lapse_rate(alt_arr,t_path,z_path,lati,loni,date_i)\n",
    "        df_0['mllr'] = copy.deepcopy(np.round(var0.values,4))\n",
    "        del(var0,alt_arr,t_path,z_path)      \n",
    "\n",
    "        # MUCAPE\n",
    "        path = inpath+'MUCAPE/MUCAPE_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['mucape'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "\n",
    "        # CIN\n",
    "        path = inpath+'CIN/CIN_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['mlcin'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "\n",
    "        # FL\n",
    "        path = inpath+'FL/FL_'+era5_datestr+'.nc'\n",
    "        var0 = calc_2D_var(path,lati,loni,date_i)\n",
    "        df_0['fl'] = copy.deepcopy(np.round(var0,4))\n",
    "        del(var0,path)             \n",
    "\n",
    "        #print('DFO')\n",
    "        #print(df_0)\n",
    "        #print('_________________________________')\n",
    "        if cntm == 0:\n",
    "            data_all = copy.deepcopy(df_0)\n",
    "        else:\n",
    "            data_all = pd.concat([data_all,df_0])\n",
    "\n",
    "        cntm = cntm + 1\n",
    "file = open(pickpath+filename, 'wb')\n",
    "# dump information to that file\n",
    "pickle.dump(data_all, file)\n",
    "file.close() # close the file            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "13d696d0-fc43-4925-a1e7-80179881f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = open(pickpath+filename, 'wb')\n",
    "## dump information to that file\n",
    "#pickle.dump(data_all, file)\n",
    "#file.close() # close the file            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63957266-5086-4d43-8f72-f5c90b12af9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85a44e4-fdeb-407c-b7a3-94ad20421967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5e0a24-6934-4f1d-93f1-0a61fa8281a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8fd7b-8fa7-4bc2-965e-bf1a4fc076f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b975818-4dfe-47da-9166-4eb4ec9bec01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28479586-503c-47fb-a73d-97b6cc927d37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea363c-49ce-4c4f-8dfb-1b37669e0052",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4c7ad9-b4fe-4650-82b8-0eae2ad6fadc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c40b701-947c-49ce-a12b-74e06242f8ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbefd56a-7662-4128-a72d-6e4dc6b17f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19efd166-b986-49d3-ad37-c166c8cc192c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a7d73b-beda-4ea3-8b6e-3357e208840b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e4650-b51c-4a1f-be7b-bcb04e80d3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f965e4ef-255f-4b7a-8a34-c2e09b88f192",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f26edb0-81c6-4430-8e2a-69d1f9c6170c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c5ea50-223d-40ab-9b61-ee8085a16f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca9f835-bb48-4ad5-8fa8-5a1a1df343d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b878db3-5ff7-4aad-a435-80aed59fbb3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8682d952-55b3-4773-98d2-087532f3e2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20029af-860f-4a5e-9c0a-289ec1b7a79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49580e51-1b72-4276-bb82-7db9ad3bf830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic",
   "language": "python",
   "name": "basic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
